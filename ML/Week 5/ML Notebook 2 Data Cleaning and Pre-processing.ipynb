{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koL3A6AYfCXz"
   },
   "source": [
    "# Notebook 2: Data Cleaning and Pre-processing\n",
    "Organized and prepared by Christopher Monterola, updated by Kenneth Co and Gino Borja\n",
    "\n",
    "This notebook was conceptualized, organized, and primarily prepared for the **Machine Learning** course.\n",
    "\n",
    "### This notebook uses the following reference:\n",
    "- Python Machine Learning by Sebastian Raschka and Vahid Mirjalili,\n",
    "Second Edition, September 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Xhpb2fffCX1"
   },
   "source": [
    "# General Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vweo6Tiq7vFe"
   },
   "source": [
    "## Dealing with missing data\n",
    "It is not uncommon in real-world applications for our samples to be missing one\n",
    "or more values for various reasons. There could have been an error in the data\n",
    "collection process, certain measurements are not applicable, or particular fields could\n",
    "have been simply left blank in a survey, for example. We typically see missing values\n",
    "as the blank spaces in our data table or as placeholder strings such as NaN, which\n",
    "stands for not a number, or NULL (a commonly used indicator of unknown values in\n",
    "relational databases).\n",
    "\n",
    "Unfortunately, most computational tools are unable to handle such missing values,\n",
    "or produce unpredictable results if we simply ignore them. Therefore, it is crucial\n",
    "that we take care of those missing values before we proceed with further analyses.\n",
    "In this section, we will work through several practical techniques for dealing with\n",
    "missing values by removing entries from our dataset or imputing missing values\n",
    "from other samples and features.\n",
    "\n",
    "*The quality of the data and the amount of useful information that it contains are key factors that determine how well a machine learning algorithm can learn. Therefore, it is absolutely critical that we make sure to examine and preprocess a dataset before we feed it to a learning algorithm. In this notebook, we will discuss the essential data preprocessing techniques that will help us build good machine learning models.*\n",
    "\n",
    "The topics that we will cover here are as follows:\n",
    "- Removing and imputing missing values from the dataset\n",
    "- Getting categorical data into shape for machine learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYk8Ln72fCX2"
   },
   "source": [
    "# Step 1. Identify missing values in tabular data\n",
    "\n",
    "But before we discuss several techniques for dealing with missing values, let's create a simple example data frame from a Comma-separated Values (CSV) file to get a better grasp of the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 863,
     "status": "ok",
     "timestamp": 1713851519557,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "7UsIMc2LfCX3",
    "outputId": "6ef39ffa-685b-429c-dea7-458928a5e508"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "csv_data = \\\n",
    " '''A,B,C,D\n",
    "1.0,2.0,3.0,4.0\n",
    "5.0,6.0,,8.0\n",
    "10.0,11.0,12.0,'''\n",
    "\n",
    "df = pd.read_csv(StringIO(csv_data))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3phHoG7yfCX5"
   },
   "source": [
    "Using the preceding code, we read CSV-formatted data into a pandas DataFrame\n",
    "via the read_csv function and noticed that the two missing cells were replaced by\n",
    "NaN. The StringIO function in the preceding code example was simply used for the\n",
    "purposes of illustration. It allows us to read the string assigned to csv_data into a\n",
    "pandas DataFrame as if it was a regular CSV file on our hard drive.\n",
    "For a larger DataFrame, it can be tedious to look for missing values manually; in this\n",
    "case, we can use the isnull method to return a DataFrame with Boolean values that\n",
    "indicate whether a cell contains a numeric value (False) or if data is missing (True).\n",
    "Using the sum method, we can then return the number of missing values per column\n",
    "as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1713851519558,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "xQ5EJVDafCX5",
    "outputId": "7074aa0b-648d-4764-aa48-6fa2e186a71e"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNlZ_hl6fCX6"
   },
   "source": [
    "This way, we can count the number of missing values per column; in the following\n",
    "subsections, we will take a look at different strategies for how to deal with this\n",
    "missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnbWyd1NfCX7"
   },
   "source": [
    "# Step 2. Eliminate samples or features with missing values\n",
    "\n",
    "One of the easiest ways to deal with missing data is to simply remove the\n",
    "corresponding features (columns) or samples (rows) from the dataset entirely; rows\n",
    "with missing values can be easily dropped via the dropna method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1713851519558,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "L5UVmGGPfCX7",
    "outputId": "f9b4089d-7009-47d1-8ac0-74b014505827"
   },
   "outputs": [],
   "source": [
    "df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDwD4bFffCX8"
   },
   "source": [
    "Similarly, we can drop columns that have at least one NaN in any row by setting the\n",
    "axis argument to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1713851519558,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "1T-nhmLYfCX8",
    "outputId": "5575387f-4d07-45f1-c1dd-2cdab1dfd2c0"
   },
   "outputs": [],
   "source": [
    "df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "au9NTXejfCX9"
   },
   "source": [
    "The dropna method supports several additional parameters that can come in handy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1713851519917,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "W2kT4mucfCX9",
    "outputId": "cce3439d-de50-45c7-aecb-72243696d759"
   },
   "outputs": [],
   "source": [
    "# only drop rows where all columns are NaN\n",
    "#(returns the whole array here since we don't have a row with where all values are NaN\n",
    "\n",
    "df.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1713851519917,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "ouVTsC4YfCX9",
    "outputId": "796d216a-1dab-41ed-b668-c3387878ae78"
   },
   "outputs": [],
   "source": [
    "# drop rows that have less than 4 real values\n",
    "df.dropna(thresh=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1713851519917,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "PjwJR0DVfCX-",
    "outputId": "eae53fdd-7746-41c5-c43c-efae86ac4b5d"
   },
   "outputs": [],
   "source": [
    "# only drop rows where NaN appear in specific columns (here: 'C')\n",
    "df.dropna(subset=['C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZP6kNU0fCX-"
   },
   "source": [
    "Although the removal of missing data seems to be a convenient approach, it also\n",
    "comes with certain disadvantages; for example, we may end up removing too\n",
    "many samples, which will make a reliable analysis impossible. Or, if we remove too\n",
    "many feature columns, we will run the risk of losing valuable information that our\n",
    "classifier needs to discriminate between classes. In the next section, we will thus\n",
    "look at one of the most commonly used alternatives for dealing with missing values:\n",
    "interpolation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njCqKbEEfCX-"
   },
   "source": [
    "# Step 3. Imputing missing values\n",
    "\n",
    "Often, the removal of samples or dropping of entire feature columns is simply not feasible, because we might lose too much valuable data. In this case, we can use different interpolation techniques to estimate the missing values from the other training samples in our dataset. One of the most common interpolation techniques is mean imputation, where we simply replace the missing value with the mean value of the entire feature column. A convenient way to achieve this is by using the Imputer class from scikit-learn, as shown in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1570,
     "status": "ok",
     "timestamp": 1713851521482,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "5UaPRHyAfCX_",
    "outputId": "9b3f3b41-178b-4de8-b780-3c4c994cbbcf"
   },
   "outputs": [],
   "source": [
    "csv_data = \\\n",
    " '''A,B,C,D\n",
    "1.0,2.0,3.0,4.0\n",
    "5.0,6.0,,8.0\n",
    "5.0,6.0,7,8.0\n",
    "10.0,11.0,7.0,'''\n",
    "\n",
    "df = pd.read_csv(StringIO(csv_data))\n",
    "\n",
    "print(df)\n",
    "\n",
    "import numpy as np\n",
    "#from sklearn.preprocessing import Imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "imr = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "# alternatively strategy can be 'most_frequent', 'median'\n",
    "imr = imr.fit(df.values)\n",
    "imputed_data = imr.transform(df.values)\n",
    "imputed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUnRk32afCX_"
   },
   "source": [
    "Here, we replaced each NaN value with the corresponding mean, which is separately\n",
    "calculated for each feature column. Other options for the strategy parameter are median or\n",
    "most_frequent, where the latter replaces the missing values with the most frequent\n",
    "values. This is useful for imputing categorical feature values, for example, a feature\n",
    "column that stores an encoding of color names, such as red, green, and blue, and we\n",
    "will encounter examples of such data later in this chapter. How about if data is mixed categorical and numerical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1713851521482,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "3sN8ncA_fCX_",
    "outputId": "9ee0a701-baa4-43a2-e4e8-2e8a89eda960"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "\n",
    "        Columns of dtype object are imputed with the most frequent value\n",
    "        in column.\n",
    "\n",
    "        Columns of other types are imputed with mean of column.\n",
    "\n",
    "        \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n",
    "            index=X.columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "\n",
    "data = [\n",
    "    ['a', 1, 2],\n",
    "    ['b', 1, 1],\n",
    "    ['b', 2, 2],\n",
    "    [np.nan, np.nan, np.nan]\n",
    "]\n",
    "\n",
    "X = pd.DataFrame(data)\n",
    "xt = DataFrameImputer().fit_transform(X)\n",
    "\n",
    "print('before...')\n",
    "print(X)\n",
    "print('after...')\n",
    "print(xt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnUayHsOfCYA"
   },
   "source": [
    "# Step 4. Handling categorical data\n",
    "Most machine learning algorithms work with numerical values only. However, it is not uncommon that real-world datasets contain one or more categorical feature columns. In this section, we will make use of simple yet effective examples to see how we deal with this type of data in numerical computing libraries.\n",
    "\n",
    "### *Nominal and ordinal features*\n",
    "When we are talking about categorical data, we have to further distinguish between nominal and ordinal features. Ordinal features can be understood as categorical values that can be sorted or ordered. For example, t-shirt size would be an ordinal feature, because we can define an order XL > L > M. In contrast, nominal features don't imply any order and, to continue with the previous example, we could think of t-shirt color as a nominal feature since it typically doesn't make sense to say that, for example, red is larger than blue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXYCtuJxfCYA"
   },
   "source": [
    "### Creating an example dataset\n",
    "\n",
    "Before we explore different techniques to handle such categorical data, let's create a\n",
    "new DataFrame to illustrate the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 421,
     "status": "ok",
     "timestamp": 1713851521901,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "IomRy4UNfCYB",
    "outputId": "44095a48-753f-4929-becb-40cb7c654bcc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([\n",
    "['green', 'M', 10.1, 'class1'],\n",
    "['red', 'L', 13.5, 'class2'],\n",
    "['blue', 'XL', 15.3, 'class1']])\n",
    "df.columns = ['color', 'size', 'price', 'classlabel']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GnZI7ukfCYB"
   },
   "source": [
    "As we can see in the preceding output, the newly created DataFrame contains a\n",
    "nominal feature (color), an ordinal feature (size), and a numerical feature (price)\n",
    "column. The class labels (assuming that we created a dataset for a supervised\n",
    "learning task) are stored in the last column. The learning algorithms for classification\n",
    "that we discuss in this book do not use ordinal information in class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3mljBuIfCYB"
   },
   "source": [
    "## Step 4.1 Mapping ordinal features\n",
    "\n",
    "To make sure that the learning algorithm interprets the ordinal features correctly,\n",
    "we need to convert the categorical string values into integers. Unfortunately, there is\n",
    "no convenient function that can automatically derive the correct order of the labels\n",
    "of our size feature, so we have to define the mapping manually. In the following\n",
    "simple example, let's assume that we know the numerical difference between\n",
    "features, for example, XL = L +1 = M + 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1713851521901,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "vmlA1210fCYB",
    "outputId": "5e8a0bba-2518-470b-c403-66fa075c5094"
   },
   "outputs": [],
   "source": [
    "size_mapping = {\n",
    "'XL': 2,\n",
    "'L': 1,\n",
    "'M': 0}\n",
    "df['size'] = df['size'].map(size_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7q_e5HGqfCYC"
   },
   "source": [
    "If we want to transform the integer values back to the original string representation\n",
    "at a later stage, we can simply define a reverse-mapping dictionary inv_size_\n",
    "mapping = {v: k for k, v in size_mapping.items()} that can then be\n",
    "used via the pandas map method on the transformed feature column, similar to\n",
    "the size_mapping dictionary that we used previously. We can use it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1713851521901,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "qrU5gs0JfCYC",
    "outputId": "40083a96-d0cb-4c5f-9088-9d109b4df06b"
   },
   "outputs": [],
   "source": [
    "inv_size_mapping = {v: k for k, v in size_mapping.items()}\n",
    "df['size'].map(inv_size_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9YjdOu3fCYC"
   },
   "source": [
    "## Step 4.2 Encoding class labels\n",
    "Many machine learning libraries require that class labels are encoded as integer\n",
    "values. Although most estimators for classification in scikit-learn convert class\n",
    "labels to integers internally, it is considered good practice to provide class labels as\n",
    "integer arrays to avoid technical glitches. To encode the class labels, we can use an\n",
    "approach similar to the mapping of ordinal features discussed previously. We need\n",
    "to remember that class labels are not ordinal, and it doesn't matter which integer\n",
    "number we assign to a particular string label. Thus, we can simply enumerate the\n",
    "class labels, starting at 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1713851521901,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "VSmPKw3VfCYC",
    "outputId": "b7cf6180-17d4-473b-ae7f-cd459e14fd52"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class_mapping = {label:idx for idx,label in enumerate(np.unique(df['classlabel']))}\n",
    "class_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5IPoYEjfCYC"
   },
   "source": [
    "Next, we can use the mapping dictionary to transform the class labels into integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1713851521901,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "1ClF7UzNfCYC",
    "outputId": "2772200e-d94f-4ed9-f6d9-4289f214c060"
   },
   "outputs": [],
   "source": [
    "df['classlabel'] = df['classlabel'].map(class_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5gnxS40fCYD"
   },
   "source": [
    "We can reverse the key-value pairs in the mapping dictionary as follows to map the\n",
    "converted class labels back to the original string representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1713851521901,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "LaTtu5QMfCYD",
    "outputId": "5ddb9a84-f1cf-4b22-c95c-d46cf61f7319"
   },
   "outputs": [],
   "source": [
    "inv_class_mapping = {v: k for k, v in class_mapping.items()}\n",
    "df['classlabel'] = df['classlabel'].map(inv_class_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6fMYzc1fCYD"
   },
   "source": [
    "Alternatively, there is a convenient LabelEncoder class directly implemented in\n",
    "scikit-learn to achieve this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1713851521901,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "m7Q1dztlfCYD",
    "outputId": "2804f433-90bd-4121-d4e7-cce67a9e5da7"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "class_le = LabelEncoder()\n",
    "y = class_le.fit_transform(df['classlabel'].values)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aL6qP-00fCYD"
   },
   "source": [
    "Note that the fit_transform method is just a shortcut for calling fit and\n",
    "transform separately, and we can use the inverse_transform method to transform\n",
    "the integer class labels back into their original string representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1713851521901,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "MQHGthbEfCYD",
    "outputId": "2334d4b4-dc9b-4feb-96c2-4e0050348ba4"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8CjFdcYfCYD"
   },
   "source": [
    "## Step 4.3 Performing one-hot encoding on nominal features\n",
    "\n",
    "In the previous section, we used a simple dictionary-mapping approach to convert\n",
    "the ordinal size feature into integers. Since scikit-learn's estimators for classification\n",
    "treat class labels as categorical data that does not imply any order (nominal), we used\n",
    "the convenient LabelEncoder to encode the string labels into integers. It may appear\n",
    "that we could use a similar approach to transform the nominal color column of our\n",
    "dataset, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1713851521901,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "ngyRzqWLfCYE",
    "outputId": "b080727f-0248-490c-f8c1-c58e2785ea9e"
   },
   "outputs": [],
   "source": [
    "X = df[['color', 'size', 'price']].values\n",
    "color_le = LabelEncoder()\n",
    "X[:, 0] = color_le.fit_transform(X[:, 0])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qc8wXOeZfCYE"
   },
   "source": [
    "After executing the preceding code, the first column of the NumPy array X now\n",
    "holds the new color values, which are encoded as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ov0Cc4grfCYE"
   },
   "source": [
    "• blue = 0\n",
    "\n",
    "• green = 1\n",
    "\n",
    "• red = 2\n",
    "\n",
    "If we stop at this point and feed the array to our classifier, we will make one of the\n",
    "most common mistakes in dealing with categorical data. Can you spot the problem?\n",
    "Although the color values don't come in any particular order, a learning algorithm\n",
    "will now assume that green is larger than blue, and red is larger than green.\n",
    "Although this assumption is incorrect, the algorithm could still produce useful\n",
    "results. However, those results would not be optimal.\n",
    "A common workaround for this problem is to use a technique called one-hot\n",
    "encoding. The idea behind this approach is to create a new dummy feature for each\n",
    "unique value in the nominal feature column. Here, we would convert the color\n",
    "feature into three new features: blue, green, and red. Binary values can then be\n",
    "used to indicate the particular color of a sample; for example, a blue sample can be\n",
    "encoded as blue=1, green=0, red=0. To perform this transformation, we can use the\n",
    "OneHotEncoder that is implemented in the scikit-learn.preprocessing module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1713851521902,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "gmjfeRkGfCYE",
    "outputId": "b865b396-3244-46e7-c8d1-8182d4402744"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ct = ColumnTransformer([(\"color\", OneHotEncoder(), [0])], remainder = 'passthrough')\n",
    "X = ct.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUmTRgnqfCYE"
   },
   "source": [
    "When we initialized the OneHotEncoder, we defined the column position of the\n",
    "variable that we want to transform via the categorical_features parameter (note\n",
    "that color is the first column in the feature matrix X). By default, the OneHotEncoder\n",
    "returns a sparse matrix when we use the transform method, and we converted the\n",
    "sparse matrix representation into a regular (dense) NumPy array for the purpose\n",
    "of visualization via the toarray method. Sparse matrices are a more efficient way\n",
    "of storing large datasets and one that is supported by many scikit-learn functions,\n",
    "which is especially useful if an array contains a lot of zeros. To omit the toarray\n",
    "step, we could alternatively initialize the encoder as OneHotEncoder(...,\n",
    "sparse=False) to return a regular NumPy array.\n",
    "\n",
    "An even more convenient way to create those dummy features via one-hot encoding\n",
    "is to use the get_dummies method implemented in pandas. Applied to a DataFrame,\n",
    "the get_dummies method will only convert string columns and leave all other\n",
    "columns unchanged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1713851521902,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "SA5xCBpMfCYE",
    "outputId": "fb537186-eec3-4a35-b510-ba43b2185bfd"
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(df[['price', 'color', 'size']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vJ_MF1LfCYE"
   },
   "source": [
    "When we are using one-hot encoding datasets, we have to keep in mind that it\n",
    "introduces multicollinearity, which can be an issue for certain methods (for instance,\n",
    "methods that require matrix inversion). If features are highly correlated, matrices are\n",
    "computationally difficult to invert, which can lead to numerically unstable estimates.\n",
    "To reduce the correlation among variables, we can simply remove one feature\n",
    "column from the one-hot encoded array. Note that we do not lose any important\n",
    "information by removing a feature column, though; for example, if we remove the\n",
    "column color_blue, the feature information is still preserved since if we observe\n",
    "color_green=0 and color_red=0, it implies that the observation must be blue.\n",
    "\n",
    "If we use the get_dummies function, we can drop the first column by passing a True\n",
    "argument to the drop_first parameter, as shown in the following code example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1713851521902,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "iXjheatlfCYF",
    "outputId": "b0689a45-94b0-47e8-83e7-07faff5144fd"
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(df[['price', 'color', 'size']], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hNNA_kkfCYF"
   },
   "source": [
    "# Step 5. Bringing features onto the same scale\n",
    "\n",
    "Feature scaling is a crucial step in our preprocessing pipeline that can easily be\n",
    "forgotten. Decision trees and random forests are two of the very few machine\n",
    "learning algorithms where we don't need to worry about feature scaling. Those\n",
    "algorithms are scale invariant. However, the majority of machine learning and\n",
    "optimization algorithms behave much better if features are on the same scale.\n",
    "\n",
    "The importance of feature scaling can be illustrated by a simple example. Let's assume that we have two features where one feature is measured on a scale from 1 to 10 and the second feature is measured on a scale from 1 to 100,000, respectively. It is intuitive to say that the algorithm will mostly be busy optimizing the weights according to the larger errors in the second feature. Another example is the k-nearest neighbors (KNN) algorithm with a Euclidean distance measure; the computed distances between samples will be dominated by the second feature axis.\n",
    "\n",
    "\n",
    "Now, there are two common approaches to bring different features onto the same\n",
    "scale: normalization and standardization. Those terms are often used quite loosely\n",
    "in different fields, and the meaning has to be derived from the context. Most often,\n",
    "normalization refers to the rescaling of the features to a range of [0, 1], which is a\n",
    "special case of min-max scaling. To normalize our data, we can simply apply the\n",
    "min-max scaling to each feature column, where the new value (i)norm x of a sample x(i)\n",
    "can be calculated as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "x^{(i)}_{norm} =\\frac{x^{(i)} -x_{min}}{x_{max} -x_{min}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8u4VU1JfCYF"
   },
   "source": [
    "Here, $x^{(i)}$ is a particular sample, $x_{min}$ is the smallest value in a feature column, and\n",
    "$x_{max}$ the largest value. The min-max scaling procedure is implemented in scikit-learn and can be used as\n",
    "follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 493,
     "status": "ok",
     "timestamp": 1713851522385,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "xiILwNX2fCYF",
    "outputId": "6a8cc0fb-316b-4800-d473-fb5b14a2c943"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "X_norm = mms.fit_transform(X)\n",
    "print(X)\n",
    "print(X_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaYYHWSJfCYF"
   },
   "source": [
    "Although normalization via min-max scaling is a commonly used technique that\n",
    "is useful when we need values in a bounded interval, standardization can be more\n",
    "practical for many machine learning algorithms, especially for optimization algorithms\n",
    "such as gradient descent. The reason is that many linear models, such as the logistic\n",
    "regression and SVM initialize the weights to 0 or small random values close\n",
    "to 0.\n",
    "\n",
    "Using standardization, we center the feature columns at mean 0 with standard\n",
    "deviation 1 so that the feature columns takes the form of a normal distribution, which\n",
    "makes it easier to learn the weights. Furthermore, standardization maintains useful\n",
    "information about outliers and makes the algorithm less sensitive to them in contrast\n",
    "to min-max scaling, which scales the data to a limited range of values.\n",
    "The procedure for standardization can be expressed by the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "x^{(i)}_{std} = \\frac{x^{(i)}-\\mu_x}{\\sigma_x}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VC_xVgzjfCYF"
   },
   "source": [
    "Here, $\\mu_x$ is the sample mean of a particular feature column and $\\sigma_x$ is the corresponding standard deviation.\n",
    "\n",
    "The following  illustrates the difference between the two commonly used feature scaling techniques, standardization and normalization, on a simple sample dataset consisting of numbers 0 to 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1713851522385,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "lt90ZaICfCYF",
    "outputId": "b6f3c88b-00f2-410e-99c4-0b87c8270d90"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "stdsc = StandardScaler()\n",
    "\n",
    "ex = np.array([[0, 1, 2, 3, 4, 5]])\n",
    "ex.T\n",
    "X_norm = mms.fit_transform(ex.T)\n",
    "print(\"X_norm\")\n",
    "print(X_norm)\n",
    "X_std=stdsc.fit_transform(ex.T)\n",
    "print(\"\\nX_std\")\n",
    "print(X_std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ihm_32lgfCYF"
   },
   "source": [
    "Note that it important to highlight that we fit the StandardScaler or the MinmaxScaler class only once—on the training data—and use those parameters to transform the test set or any new data point.\n",
    "\n",
    ">>> X_train_std = stdsc.fit_transform(X_train)\n",
    "\n",
    ">>> X_test_std = stdsc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PexANplfCYG"
   },
   "source": [
    "# **Summary**\n",
    "\n",
    "In summary, the following are the basic steps in data preprocessing/cleaning. A more tedious job includes purging categorical variables that are one and the same like: \"Apple, aple, mansanas, appl\" all into apple; such can be handled by encoding class labels but that is generally more tedious.\n",
    "\n",
    "- Step 1. Identify missing values in tabular data\n",
    "- Step 2. Eliminate samples or features with missing values OR\n",
    "- Step 3. Imputing missing values\n",
    "- Step 4. Handling categorical data\n",
    "  - 4.1 Mapping ordinal features\n",
    "  - 4.2 Encoding class labels\n",
    "  - 4.3 Performing one-hot encoding on nominal features\n",
    "- Step 5. Bringing features onto the same scale"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
